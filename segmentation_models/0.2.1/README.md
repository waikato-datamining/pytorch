# Segmentation Models

Uses [Segmentation Models](https://github.com/qubvel/segmentation_models.pytorch) ([documentation](https://smp.readthedocs.io/en/v0.2.1/)). 

Uses PyTorch 1.9.0, CUDA 11.1 and Segmentation Models 0.2.1.

Though Segmentation Models is installed via a wheel file, you can find its source code \
inside the container in:

```bash
/opt/segmentation_models
```

Additional code is located in:

```bash
/opt/segmentation_models_ext
```

## Version

Segmentation Models github repo hash:

```
a288d337821716ad67125127b5dd96a1cd833391
```

and timestamp:

```
October 26, 2021
```

## Docker

### Quick start

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Pull and run image (adjust volume mappings `-v`):

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host \
    -v /local/dir:/container/dir \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```

  **NB:** For docker versions 19.03 (`docker version`) and newer, use `--gpus=all` instead of `--runtime=nvidia`.

### Docker hub

The image is also available from [Docker hub](https://hub.docker.com/u/waikatodatamining):

```
waikatodatamining/segmentation_models:0.2.1
```

### Build local image

* Build the image from Docker file (from within /path_to/segmentation_models/0.2.1)

  ```bash
  docker build -t segmentation_models .
  ```
  
* Run the container

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host -v /local/dir:/container/dir -it segmentation_models
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container

### Pre-built images

* Build

  ```bash
  docker build -t segmentation_models:0.2.1 .
  ```
  
* Tag

  ```bash
  docker tag \
    segmentation_models:0.2.1 \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```
  
* Pull

  If image is available in aml-repo and you just want to use it, you can pull using following command and then [run](#run).

  ```bash
  docker pull public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public.aml-repo.cms.waikato.ac.nz:443
  ```
  Then tag by running:
  
  ```bash
  docker tag \
    public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1 \
    pytorch/segmentation_models:0.2.1
  ```
  
* <a name="run">Run</a>

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host \
    -v /local/dir:/container/dir -it pytorch/segmentation_models:0.2.1
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Caching

Segmentation models will download pretrained models and cache them locally. To avoid having
to download them constantly, you can the cache directory to the host machine:

* when running the container as `root`

  ```bash
  -v /some/where/cache:/root/.cache \
  ```

* when running the container as current user

  ```bash
  -v /some/where/cache:/.cache \
  ```


## Scripts

The following additional scripts are available:

* `sm_train` - for training models (calls `/opt/segmentation_models_ext/train.py`)
* `sm_predict` - for generating batch predictions on images (calls `/opt/segmentation_models_ext/predict.py`)
* `sm_predict_redis` - for generating batch predictions on images via redis backend (calls `/opt/segmentation_models_ext/predict_redis.py`)
* `sm_test_image_redis` - for uploading an image to the redis backend (calls `/opt/segmentation_models_ext/test_image_redis.py`)


## Config files
Config files can be in JSON (.json) or YAML (.yaml, .yml). 

Configuration files in JSON format to be used for the cars segmentation examples:

* [binary](examples/cars_binary.json)
* [multi-class](examples/cars_multi.json)


## Augmentations

The following Python snippets show how to generate JSON output of albumentation augmentation
pipelines, which can be copy/pasted into the config file under the `train_aug`/`test_aug` keys. 

### Train

```python
import albumentations as albu

def get_augmentation():
    _transform = [
        albu.HorizontalFlip(p=0.5),
        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),
        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),
        albu.RandomCrop(height=320, width=320, always_apply=True),
        albu.IAAAdditiveGaussianNoise(p=0.2),
        albu.IAAPerspective(p=0.5),
        albu.OneOf(
            [
                albu.CLAHE(p=1),
                albu.RandomBrightness(p=1),
                albu.RandomGamma(p=1),
            ],
            p=0.9,
        ),
        albu.OneOf(
            [
                albu.IAASharpen(p=1),
                albu.Blur(blur_limit=3, p=1),
                albu.MotionBlur(blur_limit=3, p=1),
            ],
            p=0.9,
        ),
        albu.OneOf(
            [
                albu.RandomContrast(p=1),
                albu.HueSaturationValue(p=1),
            ],
            p=0.9,
        ),
    ]
    return albu.Compose(_transform)

if __name__ == "__main__":
    albu.save(get_augmentation(), "./train_aug.json", data_format='json')
```

### Test

```python
import albumentations as albu

def get_augmentation():
    test_transform = [
        albu.PadIfNeeded(384, 480)
    ]
    return albu.Compose(test_transform)

if __name__ == "__main__":
    albu.save(get_augmentation(), "./test_aug.json", data_format='json')
```


## Examples

### Car segmentation

A simple [car segmentation example](examples/cars_segmentation.py) is included in the image 
([source](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb)):

```
cd /opt/segmentation_models_ext
python3 cars_segmentation.py
```

**NB:** This will clone the repository with the [data](https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid)
in the `/opt/segmentation_models_ext` directory. If you want to avoid re-cloning it, then copy 
the `cars_segmentation.py` script into a directory that is mapped to a directory on the host, e.g.:

```bash
docker run --runtime=nvidia -u $(id -u):$(id -g) -e USER=$USER --shm-size 8G --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -it waikatodatamining/segmentation_models:0.2.1

cp /opt/segmentation_models_ext/cars_segmentation.py
cd /workspace/
python3 cars_segmentation.py
```

### Car segmentation (multi-class)

Based on the example above, but [adapted](examples/cars_segmentation_multi.py) to train a multi-class model, predicting all 
classes in the datasets not just cars ([source](https://raw.githubusercontent.com/shirokawakita/multiclass-segmentation/main/example_camvid_multiclassB_quita.ipynb)).

```
/opt/segmentation_models_ext/cars_segmentation_multi.py
```
