# Huggingface transformers (finetune-gpt2xl)

Docker image for [Huggingface transformers](https://github.com/huggingface/transformers) 4.7.0
that contains [finetune-gpt2xl](https://github.com/Xirider/finetune-gpt2xl) support.

Uses PyTorch 1.8.1, CUDA 11.1, finetune-gpt2xl 20220924 (e0b02491d7773af34a4cedb0cc441ac81c8ad326).

## Quick start

### Inhouse registry

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Create the `cache` and `triton` directories (to house downloaded dataset and models):

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```

### Docker hub
  
* Create the `cache` and `triton` directories (to house downloaded dataset and models):

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it waikatodatamining/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```

### Build local image

* Build the image from Docker file (from within /path_to/huggingface-transformers/4.7.0_cuda11.1_finetune-gpt2xl_20220924)

  ```bash
  docker build -t huggingface-transformers .
  ```
  
* Run the container

  ```bash
  docker run --gpus=all --shm-size --net=host 8G -v /local/dir:/container/dir -it huggingface-transformers
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Publish images

### Build

```bash
docker build -t pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924 .
```

### Inhouse registry  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924 \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```

### Docker hub  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924 \
    waikatodatamining/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```
  
* Push

  ```bash
  docker push waikatodatamining/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login
  ```


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Scripts

* `gpt_text2csv` - for converting text files into CSV ones
* `train_clm` - for training a causal language model (calls `/opt/finetune-gpt2xl/run_clm.py`) 
* `gpt_predict_poll` - for generating plain text file using file-polling of JSON files with prompts
* `gpt_predict_redis` - for generating plain text from JSON prompts via redis

### Prompt format

```json
{
  "prompt": "the prompt text."
}
```


## Examples

### Data

Convert the text files for training and validation (located in `/workspace/data`) 
into CSV files:

```bash
gpt_text2csv -i /workspace/data/train.txt 
gpt_text2csv -i /workspace/data/validation.txt 
```

**NB:** Each line in the text file gets turned into a separated row in the CSV file. 


### GPT2-XL

Fine-tune a GPT2-XL model on your dataset (located in `/workspace/data/`)
and outputs the resulting model in `/workspace/output-xl`:

```bash
deepspeed --num_gpus=1 \
  /opt/finetune-gpt2xl/run_clm.py \
  --deepspeed /opt/finetune-gpt2xl/ds_config.json \
  --model_name_or_path gpt2-xl \
  --train_file /workspace/data/train.csv \
  --validation_file /workspace/data/validation.csv \
  --do_train \
  --do_eval \
  --fp16 \
  --overwrite_cache \
  --evaluation_strategy="steps" \
  --output_dir /workspace/output-xl \
  --eval_steps 200 \
  --num_train_epochs 1 \
  --gradient_accumulation_steps 2 \
  --per_device_train_batch_size 8 \
  --overwrite_output_dir
```

Or, without using deepspeed, you can just use the `train_clm` script and supply 
it with all the options of `run_clm.py`.

Generate predictions using file polling on .json prompt files:

```bash
gpt_predict_poll \
    --model_type gptneo \
    --model_path /workspace/output-xl/ \
    --length 100 \
    --fp16 \
    --use_cache \
    --do_sample \
    --prediction_in /workspace/predictions/in/ \
    --prediction_out /workspace/predictions/out/
```

### GPT-Neo

Fine-tune a GPT2-Neo model on your dataset (located in `/workspace/data/`)
and outputs the resulting model in `/workspace/output-neo`:

```bash
deepspeed --num_gpus=1 \
  /opt/finetune-gpt2xl/run_clm.py \
  --deepspeed /opt/finetune-gpt2xl/ds_config_gptneo.json \
  --model_name_or_path EleutherAI/gpt-neo-2.7B \
  --train_file /workspace/data/train.csv \
  --validation_file /workspace/data/validation.csv \
  --do_train \
  --do_eval \
  --fp16 \
  --overwrite_cache \
  --evaluation_strategy="steps" \
  --output_dir /workspace/output-neo \
  --num_train_epochs 1 \
  --eval_steps 15 \
  --gradient_accumulation_steps 2 \
  --per_device_train_batch_size 4 \
  --use_fast_tokenizer False \
  --learning_rate 5e-06 \
  --warmup_steps 10 \
  --overwrite_output_dir
```

Generate predictions using file polling on .json prompt files:

```bash
gpt_predict_poll \
    --model_type gptneo \
    --model_path /workspace/output-neo/ \
    --length 100 \
    --fp16 \
    --use_cache \
    --do_sample \
    --prediction_in /workspace/predictions/in/ \
    --prediction_out /workspace/predictions/out/
```
