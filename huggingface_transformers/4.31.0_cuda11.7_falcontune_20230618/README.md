# Huggingface transformers (falcontune)

Docker image for [Huggingface transformers](https://github.com/huggingface/transformers) 4.31.0
that contains [falcontune](https://github.com/rmihaylov/falcontune) support.

Uses PyTorch 2.0.1, CUDA 11.7, falcontune 20230618 (6bd029e5a89f58c4eea9056ee1c86127e1200876).

## Quick start

### Inhouse registry

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Create the `cache` directory (to house downloaded dataset and models):

  ```bash
  mkdir cache config
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/config:/.config \
    -v `pwd`/cache:/.cache \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```

### Docker hub
  
* Create the `cache` directory (to house downloaded dataset and models):

  ```bash
  mkdir cache config
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/config:/.config \
    -v `pwd`/cache:/.cache \
    -it waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```

### Build local image

* Build the image from Docker file (from within /path_to/huggingface-transformers/4.31.0_cuda11.7_falcontune_20230618)

  ```bash
  docker build -t huggingface-transformers .
  ```
  
* Run the container

  ```bash
  docker run --gpus=all --shm-size --net=host 8G -v /local/dir:/container/dir -it huggingface-transformers
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Publish images

### Build

```bash
docker build -t pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618 .
```

### Inhouse registry  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618 \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```

### Docker hub  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618 \
    waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```
  
* Push

  ```bash
  docker push waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login
  ```


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Scripts

* `wandb disabled` - to disable wandb logging (and prompt asking to log in)
* `falcontune finetune` - for fine-tuning a falcon model
* `falcontune generate` - for using a falcon model


## Examples

```bash
falcontune finetune \
    --model=falcon-7b-instruct \
    --weights=tiiuae/falcon-7b-instruct \
    --dataset=./alpaca_data_cleaned.json \
    --data_type=alpaca \
    --lora_out_dir=./falcon-7b-instruct-alpaca/ \
    --mbatch_size=1 \
    --batch_size=2 \
    --epochs=3 \
    --lr=3e-4 \
    --cutoff_len=256 \
    --lora_r=8 \
    --lora_alpha=16 \
    --lora_dropout=0.05 \
    --warmup_steps=5 \
    --save_steps=50 \
    --save_total_limit=3 \
    --logging_steps=5 \
    --target_modules='["query_key_value"]'
```

The above command will download the model and use LoRA to fine-tune 
the quantized model. The final adapters and the checkpoints will be 
saved in `falcon-7b-instruct-alpaca` and available for generation as 
follows:

```bash
falcontune generate \
    --interactive \
    --model falcon-7b-instruct \
    --weights tiiuae/falcon-7b-instruct \
    --lora_apply_dir falcon-7b-instruct-alpaca \
    --max_new_tokens 50 \
    --use_cache \
    --do_sample \
    --instruction "How to prepare pasta?"
```

**Notes:** 

* Data for the above example can be obtained from [AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned).
* More examples can be found [here](https://github.com/rmihaylov/falcontune#finetune-a-base-model).
